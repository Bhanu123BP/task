{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgjM1QwVmVZ",
        "outputId": "8785d51a-7723-45e5-f910-e9b4c983f0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.10/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (from fitz) (7.1.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.10/dist-packages (from fitz) (1.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.10/dist-packages (from fitz) (1.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.2.0)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (6.4.5)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.0.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.19.3)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.4.3)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.2.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.28)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (5.3.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.10/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install sentence-transformers faiss-cpu langchain requests fitz numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuoU4giGV4Aa",
        "outputId": "c701b19b-1ca7-40ed-82a1-655c8b295626"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the libraries and packages required\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fitz  # PyMuPDF\n",
        "import requests\n",
        "import io\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Explanation:\n",
        "#1.The SentenceTransformer model (all-MiniLM-L6-v2) is used to generate embeddings for textual data.\n",
        "#2.This is a lightweight transformer-based model optimized for semantic similarity tasks.\n",
        "\n",
        "# Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    if pdf_path.startswith(\"http\"):  # Handle URLs\n",
        "        response = requests.get(pdf_path)\n",
        "        response.raise_for_status()  # Raise an exception if download fails\n",
        "        pdf_data = io.BytesIO(response.content)\n",
        "        doc = fitz.open(stream=pdf_data, filetype=\"pdf\")  # Open from bytes\n",
        "    else:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "#Explanation:\n",
        "#1.If the pdf_path starts with \"http\", the PDF is fetched via HTTP, converted to a byte stream, and processed.\n",
        "#2.For local files, it directly reads the PDF using fitz.\n",
        "\n",
        "# Function to chunk text into smaller pieces for embeddings\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "#Explanation:\n",
        "#1.Splits the extracted text into smaller chunks (default size: 500 words).\n",
        "#2.These smaller chunks ensure that the embeddings are more manageable and relevant for semantic search.\n",
        "\n",
        "# Function to create embeddings for chunks\n",
        "def create_embeddings(chunks, embeddings_cache_path=\"embeddings.pkl\"):\n",
        "    if os.path.exists(embeddings_cache_path):\n",
        "        with open(embeddings_cache_path, \"rb\") as f:\n",
        "            embeddings = pickle.load(f)\n",
        "        return embeddings\n",
        "\n",
        "    embeddings = model.encode(chunks)  # Generate embeddings using SentenceTransformers model\n",
        "\n",
        "    # Cache embeddings to a file\n",
        "    with open(embeddings_cache_path, \"wb\") as f:\n",
        "        pickle.dump(embeddings, f)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "#Explanation:\n",
        "#1.Purpose: Converts text chunks into dense vector representations (embeddings).\n",
        "#2.If embeddings are cached (stored as a .pkl file), it loads them to save computation time.\n",
        "#3.Otherwise, embeddings are created using the SentenceTransformer model and stored in a pickle file for future use.\n",
        "\n",
        "\n",
        "# Function to store embeddings in FAISS\n",
        "def store_embeddings_in_faiss(embeddings):\n",
        "    embedding_dim = len(embeddings[0])\n",
        "    index = faiss.IndexFlatL2(embedding_dim)  # L2 similarity\n",
        "    np_embeddings = np.array(embeddings, dtype='float32')\n",
        "    index.add(np_embeddings)\n",
        "    return index\n",
        "\n",
        "#Explanation:\n",
        "#1.Function: store_embeddings_in_faiss(embeddings)\n",
        "#2.Purpose: Adds the embeddings to a FAISS index for similarity search.\n",
        "#3.Creates an IndexFlatL2 index for L2 (Euclidean distance) similarity.\n",
        "#4.Converts the embeddings into a NumPy array (float32 type) and adds them to the index.\n",
        "\n",
        "\n",
        "# Function to perform similarity search on embeddings\n",
        "def search_embeddings(query, index, chunks, top_k=3):\n",
        "    query_embedding = model.encode([query])  # Generate embedding for the query using SentenceTransformers\n",
        "\n",
        "    # Search the FAISS index\n",
        "    query_vector = np.array(query_embedding, dtype='float32').reshape(1, -1)\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "\n",
        "    # Fetch the most relevant chunks\n",
        "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
        "    return relevant_chunks\n",
        "\n",
        "#Explanation:\n",
        "#1.Function: search_embeddings(query, index, chunks, top_k=3)\n",
        "#2.Purpose: Finds the top k text chunks most relevant to the user’s query.\n",
        "#3.Steps:\n",
        "#Encodes the user query into an embedding using the same SentenceTransformer model.\n",
        "#Performs a similarity search on the FAISS index to find the closest embeddings to the query.\n",
        "#Returns the corresponding text chunks.\n",
        "\n",
        "# Function to generate a response (simplified here without LangChain)\n",
        "def generate_response(user_query, relevant_chunks):\n",
        "    context = \"\\n\".join(relevant_chunks)  # Combine the relevant chunks\n",
        "    response = f\"Based on the provided context, here's the response to your query: {user_query}\\n\\nContext:\\n{context}\"\n",
        "    return response\n",
        "\n",
        "#Explanation:\n",
        "#1.Function: generate_response(user_query, relevant_chunks)\n",
        "#2.Purpose: Combines the most relevant text chunks into a context and formulates a response.\n",
        "#The response structure includes:\n",
        "#The user’s query.\n",
        "#The most relevant chunks from the PDF content.\n",
        "\n",
        "# Main pipeline function\n",
        "def run_pipeline(pdf_path, user_query):\n",
        "    # Step 1: Extract and chunk text\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    # Step 2: Create and store embeddings\n",
        "    embeddings = create_embeddings(chunks)\n",
        "    index = store_embeddings_in_faiss(embeddings)\n",
        "\n",
        "    # Step 3: Retrieve relevant chunks for the query\n",
        "    relevant_chunks = search_embeddings(user_query, index, chunks)\n",
        "\n",
        "    # Step 4: Generate response\n",
        "    response = generate_response(user_query, relevant_chunks)\n",
        "    return response\n",
        "\n",
        "#Explanation:\n",
        "#Function: run_pipeline(pdf_path, user_query)\n",
        "#1.Combines all steps into a single pipeline:\n",
        "#2.Text Extraction: Extract text from the given PDF (URL or local).\n",
        "#3.Text Chunking: Split the text into manageable pieces.\n",
        "#4.Embedding Creation: Generate and/or load cached embeddings.\n",
        "#5.Index Storage: Store embeddings in FAISS for efficient retrieval.\n",
        "#6.Similarity Search: Retrieve the most relevant chunks for the user query.\n",
        "#7.Response Generation: Generate a meaningful response based on the relevant chunks.\n",
        "\n",
        "# Running the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the PDF file you want to process\n",
        "    pdf_path = \"/content/CSE AIML 4-1.pdf\"  # Replace with the actual path to your PDF\n",
        "\n",
        "    # The query that you want to ask based on the content of the PDF\n",
        "    user_query = \"From page 9 get detailed syllabus of Industrial Interent of things\"\n",
        "\n",
        "    # Run the pipeline\n",
        "    response = run_pipeline(pdf_path, user_query)\n",
        "\n",
        "    # Print the response\n",
        "    print(\"Response:\", response)\n",
        "\n",
        "#Explanation:\n",
        "#1.Input PDF:\n",
        "#A remote PDF document located at https://www.hunter.cuny.edu/dolciani/....\n",
        "#This PDF is downloaded, processed, and text is extracted.\n",
        "#User Query:\"From page 2 get the exact unemployment information based on type of degree input\"\n",
        "#The pipeline processes the query to fetch the relevant content from the PDF.\n",
        "\n",
        "#Output:The system identifies and retrieves the text chunks that are semantically most relevant to the query and includes them in the response.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWZysMWhV_x_",
        "outputId": "3b80d5aa-4dfb-4fde-d86d-5bb4a1e01cda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Based on the provided context, here's the response to your query: From page 9 get detailed syllabus of Industrial Interent of things\n",
            "\n",
            "Context:\n",
            "SVEC20 – B.TECH – COMPUTER SCIENCE AND ENGINEERING (AI&ML) 268 IV B. Tech. – I Semester (20BT70501) COMPUTER VISION (Common to CSE,CSE(AI) and CSE(AI&ML)) Int. Marks Ext. Marks Total Marks L T P C 30 70 100 3 - - 3 PRE-REQUISITES: Courses on “Transformation Techniques and Linear Algebra”, “Machine Learning” COURSE DESCRIPTION: Concepts of low-level vision, Image filtering operations, Masking, Thresholding techniques, Edge Detection, Dilation and erosion, Background subtraction, Shot boundary detection, Interactive segmentation, Clustering based segmentation, Texture, Classification, Overfitting, Receiver Operator curves, Object detection and recognition and Information Retrieval methods. COURSE OUTCOMES: After successful completion of this course, the students will be able to: CO1. Analyze image filtering operations to enhance image quality. CO2. Apply threshold techniques, morphological process and region growing methods for edge detection in images. CO3. Develop clustering based segmentation solutions for image synthesis. CO4. Synthesize and evaluate classification procedures for texture and feature analysis. CO5. Select and apply appropriate techniques for object recognition and detection in computer vision based applications. DETAILED SYLLABUS: UNIT–I: INTRODUCTION AND IMAGE ENHANCEMENT (8 Periods) The nature of the vision, Low-level vision – Gray scale versus color, Image processing operations; Basic image filtering operations – Gaussian smoothing, Median filters, Mode Filters, Rank Order Filters, Sharp and Unsharp masking. UNIT–II: THRESHOLDING AND EDGE DETECTION (10 Periods) Region-growing methods, Thresholding, Adaptive thresholding, Threshold selection – Variance-based thresholding, Entropy-based thresholding, Maximumlikelihood thresholding; Global valley approach to thresholding; Edge Detection – Template Matching Approach, 3×3 Template Operators, Canny Operator, Laplacian Operator; Dilation and erosion in binary images – Properties of dilation and erosion operators, Closing and opening. UNIT–III: SEGMENTATION BY CLUSTERING (9 Periods) Grouping and gestalt, Important applications – Background subtraction, Shot boundary detection, Interactive segmentation, Forming imaging regions; Image segmentation by clustering pixels, Segmentation, clustering and graphs – Terminology and facts for graphs, Agglomerative clustering with a graph, Divisive clustering with a graph, Normalized cuts. SVEC20 – B.TECH – COMPUTER SCIENCE AND ENGINEERING (AI&ML) 269 UNIT–IV: CLASSIFICATION AND DETECTION OF OBJECTS (11 Periods) Texture – Spots and bars, Representation, Synthesizing textures and filling holes in images, Shape from texture; Learning to classify – Using loss to determine decisions, Training error, test error and overfitting, Regularization, Error rate and Cross-validation, Receiver operating curves; Classifying images – Classifying images of single objects; Detecting objects in images – The sliding window method. UNIT–V: OBJECT RECOGNITION, APPLICATIONS (7 Periods) Object Recognition – Categorization, Selection, Feature questions, Geometric questions, Semantic questions; Applications – Classifying materials, Classifying scenes, Tracking people. Total Periods: 45 Topics for self-study are provided in the lesson plan TEXT BOOKS: 1. David A. Forsyth, Jean Ponce, Computer Vision: A Modern Approach, Pearson, 2nd Edition, 2012. 2. E. R. Davies, Computer and Machine Vision: Theory, Algorithms, Practicalities, Elsevier, 5th Edition, 2017. REFERENCE BOOKS: 1. Rafael C. Gonzalez, Richard E. Woods, Digital Image Processing, Pearson, 4th Edition, 2018. 2. William K. Pratt, Digital Image Processing, Wiley, 4th Edition, 2006. ADDITIONAL LEARNING RESOURCES: 1. https://slideplayer.com/slide/5158896/ 2. www.scs.carleton.ca/~c_shu/Courses/comp4900d/notes/PPT/lect1_intro.ppt CO-PO-PSO Mapping Table: Course Outcome Program Outcomes Program Specific Outcomes PO1\n",
            "PO2 PO3 PO4 PO5 PO6 PO7 PO8 PO9 PO10 PO11 PO12 PSO1 PSO2 PSO3 PSO4 CO1 3 3 - - - - - - - - - - - - - 3 CO2 3 2 - - - - - - - - - - - - - 3 CO3 2 3 1 - - - - - - - - - - - - 3 CO4 2 3 1 - - - - - - - - - - - - 3 CO5 2 2 - - 1 1 - - - - - - - - - 3 Correlation Level: 3-High; 2-Medium; 1-Low SVEC20 – B.TECH – COMPUTER SCIENCE AND ENGINEERING (AI&ML) 270 IV B. Tech. - I Semester (20BT73101) DEEP LEARNING ARCHITECTURES (Common to CSE(AI) and CSE(AI&ML)) Int. Marks Ext. Marks Total Marks L T P C 30 70 100 3 - - 3 PRE-REQUISITES: A course on “Machine Learning”. COURSE DESCRIPTION: Machine learning with shallow neural networks; Training deep neural networks; Backpropagation; Gradient based strategies; Teaching deep learners; Recurrent neural networks; Applications of Recurrent neural networks;Convolutional Architectures-AlexNet; VGG,GoogleNet; ResNet COURSE OUTCOMES: After successful completion of this course, the students will be able to: CO1. Acquire working knowledge of neural networks and explore the different parameters of the network. CO2. Construct a generative model for learning probability distribution using RBM. CO3. Analyze temporal sequential input data using gated memory based neural units. CO4. Utilize Convolutional Neural Network for analyzing visual imagery and utilize transfer learning approaches for reducing the training efforts. CO5. Applyencoder-decoder architecture for image denoising, and learning representation of a set of data. DETAILED SYLLABUS: UNIT–I: MACHINE LEARNING WITH SHALLOW NEURAL NETWORKS (9 Periods) Neural Architectures for Binary Classification Models, Neural Architectures for Multiclass Models, Backpropagated Saliency for Feature Selection, Autoencoders- Basic Principles, Nonlinear Activations, Deep Autoencoders, Application to Outlier Detection. UNIT–II: TRAINING DEEP NEURAL NETWORKS (9 Periods) Backpropagation: Backpropagation with the Computational Graph Abstraction, Dynamic Programming,Backpropagation with Post-Activation and Pre-Activation variables, Examples of Updates for Various Activations, Loss Functions on Multiple Output Nodes and Hidden Nodes, Backpropagation Tricks for Handling Shared Weights; Setup and Initialization Issues. Gradient based Strategies: Learning Rate Decay, Momentum-Based Learning, Parameter-Specific Learning Rates, Cliffs and Higher-Order Instability, Gradient Clipping, Second-Order Derivatives, Polyak Averaging, Local and Spurious Minima. UNIT–III: TEACHING DEEP LEARNERS (9 Periods) The Bias-Variance Trade-Off, Generalization Issues in Model Tuning and Evaluation, Penalty-Based Regularization, Ensemble Methods, Early Stopping, Unsupervised Pretraining, Regularization in Unsupervised Applications. SVEC20 – B.TECH – COMPUTER SCIENCE AND ENGINEERING (AI&ML) 271 UNIT–IV: RECURRENT NEURAL NETWORKS (8 Periods) Recurrent Neural Networks: Expressiveness of Recurrent Networks, The Architecture of Recurrent Neural Networks, The Challenges of Training Recurrent Networks . Long Short-Term Memory (LSTM), Gated Recurrent Units (GRUs). Applications of Recurrent Neural Networks: Automatic Image Captioning, Sequence- to-Sequence Learning and Machine Translation, Sentence-Level Classification, Time-Series Forecasting and Prediction, End-to-End Speech Recognition. UNIT–V: CONVOLUTIONAL NEURAL NETWORKS (10 Periods) The Basic Structure of a Convolutional Network, Training a Convolutional Network, Convolutional Architectures-AlexNet,VGG,GoogleNet,ResNet, Visualizing the Features of a Trained Network, Convolutional Autoencoders,\n",
            "Free software, Examples of free software, Free software license provider, Free software vs open source software, Public domain, History, Proprietary vs open source licensing model, Why companies use/don’t use open source software. UNIT–II: PRINCIPLES AND OPEN SOURCE METHODOLOGY (5 Periods) Open source initiatives, Open standards principles, Methodologies, Philosophy, Software freedom, Open source software development, Zero marginal cost, Income generation opportunities, Internationalization. UNIT–III: CASE STUDIES (8 Periods) Apache, Berkeley Software Distribution, Linux, Mozilla Firefox, Wikipedia, GNU compiler collection. UNIT–IV: OPEN SOURCE PROJECTS AND LICENSING (7 Periods) Open Source Projects: Starting and maintaining an open source project, Open source hardware, Open Source design, Open Source Teaching (OST), Open Source Media. SVEC20 – B.TECH – COMPUTER SCIENCE AND ENGINEERING (AI&ML) 310 Licensing: Licenses, Copyright, Copyleft, Patent, Creative Commons, Attribution 2.5 India (CC BY 2.5 IN). UNIT–V: OPEN SOURCE ETHICS (5 Periods) Open source vs closed source, Open source government, Ethics of open source, Social and financial impacts of open source technology, Shared software, Shared source. Total Periods: 30 Topics for self-study are provided in the lesson plan TEXT BOOKS: 1. Kailash Vadera, Bhavyesh Gandhi, Open Source Technology, University Science Press, 2009. 2. Moreno Muffatto, Open Source: A Multidisciplinary Approach, Imperial College Press, 2006. REFERENCE BOOKS: 1. Andrew M. St. Laurent, Understanding Open Source and Free Software Licensing, O’Reilly, 2004. 2. Paul Kavanagh, Open Source Software: Implementation and Management, Elsevier, 2004. ADDITIONAL LEARNING RESOURCES: 1. VM Brasseur, Forge Your Future with Open Source: Build Your Skills, Build Your Network, Build the Future of Technology, 2018. 2. Fadi P. Deek and James A. M. McHugh, Open Source Technology and Policy, Cambridge University Press, 2008. 3. Open Source Technology: Concepts, Methodologies, Tools and Applications, IGI- Global, 2014. CO-PO-PSO Mapping Table: Course Outcome Program Outcomes Program Specific Outcomes PO1 PO2 PO3 PO4 PO5 PO6 PO7 PO8 PO9 PO10 PO11 PO12 PSO1 PSO2 PSO3 PSO4 CO1 3 - - - - - - - - - - 3 - - - CO2 3 - - - - 3 - - - - - - 3 - - - CO3 - - - - - - 3 - - - - 3 - - - CO4 - - - - - 3 - - - - - - 3 - - - Correlation Level: 3-High; 2-Medium; 1-Low\n"
          ]
        }
      ]
    }
  ]
}